{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: The truncated power basis generates cubic regression splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x \\leqslant \\xi$ then $(x-\\xi)_+^3 = 0$. So if we take $a=1 = \\beta_0, b_1 = \\beta_1, c_1 = \\beta_2, d_1 = \\beta_3$, then $f(x) = f_1(x)$ for all $x \\leqslant \\xi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x > \\xi$ then $(x-\\xi)_+^3 = (x-\\xi)^3$. Then expanding $f(x)$\n",
    "\n",
    "$$f(x) = (\\beta_0 - \\beta_4 \\xi^3) + (\\beta_1 + 3\\beta_4\\xi^2)x + (\\beta_2 - 3\\beta_4\\xi)x^2 + (\\beta_3 + \\beta_4)x^3$$\n",
    "\n",
    "we see \n",
    "\n",
    "\\begin{align*}\n",
    "a_1 &= \\beta_0 - \\beta_4 \\xi^3\\\\\n",
    "b_1 &= \\beta_1 + 3\\beta_4\\xi^2\\\\\n",
    "c_1 &= \\beta_2 - 3\\beta_4\\xi\\\\\n",
    "d_1 &= \\beta_3 + \\beta_4\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "f_2(\\xi) &= (\\beta_0 - \\beta_4 \\xi^3) + (\\beta_1 + 3\\beta_4\\xi^2)\\xi + (\\beta_2 - 3\\beta_4\\xi)\\xi^2 + (\\beta_3 + \\beta_4)\\xi^3\\\\\n",
    "&= \\beta_0 - \\beta_4\\xi^3 +\\beta_1\\xi + 3\\beta_4\\xi^3 + \\beta_2\\xi^2 - 3\\beta_4\\xi^3 + \\beta_3\\xi^3 + \\beta_4\\xi^3\\\\\n",
    "&= \\beta_0 + \\beta_1\\xi + \\beta_2\\xi^2 + \\beta_3\\xi^3\\\\\n",
    "&= f_1(\\xi)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "f'_2(\\xi) &=  (\\beta_1 + 3\\beta_4\\xi^2) + 2(\\beta_2 - 3\\beta_4\\xi)\\xi + 3(\\beta_3 + \\beta_4)\\xi^2\\\\\n",
    "&= \\beta_1 + 3\\beta_4\\xi^2 + 2\\beta_2\\xi - 6\\beta_4\\xi^2 + 3\\beta_3\\xi^2 + 3\\beta_4\\xi^2\\\\\n",
    "&= \\beta_1 + 2\\beta_2\\xi + 3\\beta_3\\xi^2\\\\\n",
    "&= f'_1(\\xi)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "f''_2(\\xi) &= 2(\\beta_2 - 3\\beta_4\\xi) + 6(\\beta_3 + \\beta_4)\\xi\\\\\n",
    "&= 2\\beta_2 + 6\\beta_4\\xi\\\\\n",
    "&= f''_1(\\xi)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Alternative roughness penalties for smoothing splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're just going to describe the solutions instead of drawing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\lambda = \\infty$ and $m = 0$, then the integral term is minimized by a zero integral, hence by $g^{(0)} = g = 0$. So $\\hat{g}$ is the zero function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the integral is minimized by $g' = 0$, so $\\hat{g}$ is a constant function. Likely $g = \\beta_0 = \\overline{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the smoothing spline discussed in the chapter -- $\\hat{g}$ is the ordinary least squares line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the integral penalty forces $g^{(3)} = 0$, so $g$ is necessarily polynomial of degree leqslant 2. It's clear that it necessarily has degree 2 -- one can imagine cases where a linear $\\hat{g}$ minimizes the RSS term, and cases where a quadratic does. In general, a quadratic has more freedom, so averaging over datasets, the quadratic will have smaller RSS, and hence $\\hat{g}$ will be quadratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the $m = 3$ condition is irrelevant. Now we are minimizing the RSS over *all* functions $g$, so $\\hat{g}$ will be any function which has zero RSS (i.e. which has $\\hat{g}(x_i) = y_i$ for all $i$). For example, the interpolation spline, or a step (piecewise constant) function which passes through the y_i will have zero RSS. Such a function isn't unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Example basis function model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Example basis function model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Comparing smoothing splines with different roughness penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\lambda \\rightarrow \\infty$, the integral much approach 0. Then $\\hat{g}_1$ approaches a polynomial of degree at most 2, and $\\hat{g}_2$ approaches a polynomial of degree 3. Since $\\text{deg}(\\hat{g}_1) \\leqslant \\text{deg}(\\hat{g}_2)$, we expect $\\text{RSS}_{train}(\\hat{g}_1) \\geqslant \\text{RSS}_{train}(\\hat{g}_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\text{deg}(\\hat{g}_1) \\leqslant \\text{deg}(\\hat{g}_2)$, we expect $\\text{RSS}_{test}(\\hat{g}_1) \\leqslant \\text{RSS}_{test}(\\hat{g}_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\lambda = 0$, the roughness penalty vanishes and both $\\hat{g}_1, \\hat{g}^2$ can be any function with zero RSS (see [Exercise 2 e.](#Exercise-2:-Alternative-roughness-penalties-for-smoothing-splines) above). Such functions aren't uniquely defined, so in the absence of a rule for choosing $\\hat{g}_1, \\hat{g}_2$ in this case, we can't answer the question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "857px",
    "left": "878px",
    "top": "132px",
    "width": "229px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
